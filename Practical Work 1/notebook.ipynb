{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtMk6oKXNHGE"
   },
   "source": [
    "# Trabalho Prático 1 - Aplicação de redes neuronais a um novo dataset\n",
    "Objetivo: Explorar o uso de redes neuronais (keras) e comparação dos resultados com o uso de [AutoKeras](https://autokeras.com/).\n",
    "\n",
    "Tipo de tarefa machine learning: Classificação Binária.\n",
    "\n",
    "Dataset utilizado: [Titanic](https://public.opendatasoft.com/explore/dataset/titanic-passengers)\n",
    "\n",
    "Repositório: [GitHub](https://github.com/spamz23/Applied-Deep-Learning/tree/005e14f8c27e176ac310183a5a2216c8838f2abc/Practical%20Work%201)\n",
    "\n",
    "Trabalho realizado por:\n",
    "* [Diogo Silva](https://github.com/spamz23)\n",
    "* [Bruno Silva](https://github.com/brunosilva5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir64lObGN1-2"
   },
   "source": [
    "# Instalar dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K_n9v8SVNPXZ"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "# Limpar output\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VIqJzxKyC19"
   },
   "source": [
    "# Resultados Reproduzíveis\n",
    "Para uma comparação justa entre os resultados de 2 técnicas diferentes, é importante que os resultados de ambas as técnicas sejam **reproduzíveis**. Para isto podemos fazer \"seed\" nos geradores de números aleatórios usados no backend da biblioteca `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T4qpVl8oxf7O"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fe67a438d10b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "global_seed = 0 # pode ser qualquer número\n",
    "from numpy.random import seed\n",
    "seed(global_seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(global_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39Dm-ILMNHGF"
   },
   "source": [
    "## 1. Fazer load dos dados\n",
    "Para fazer load e tratar os dados, será utilizada a biblioteca pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJJP3eHENHGH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "index = \"PassengerId\"\n",
    "dataset = pd.read_excel(\"titanic-passengers.xlsx\", index_col=\"PassengerId\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXkVU-onNHGL"
   },
   "source": [
    "### Primeiro é importante analisar e perceber a estrutura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "nfI9sHo9NHGM",
    "outputId": "c26b7d55-1976-406b-e06b-1bfb3e683c4b"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLRDMIQgNHGS"
   },
   "source": [
    "### Informações obtidas\n",
    "Na tabela acima podemos observar que o dataset de treino é composto por 891 linhas (cada linha corresponde a uma pessoa), e por 11 colunas.\n",
    "Podemos então tirar as seguintes conclusões:\n",
    "1. Existem várias colunas com variáveis categóricas (`Name`, `Sex`, `Ticket`, `Cabin`, `Embarked`). Todas estas terão que ser convertidas para números para se poderem usar na rede neuronal.\n",
    "2. Existem algumas colunas com valores `NaN`, ou seja valores em falta. Todas estas também terão que se preencher, ou então remover do dataset.\n",
    "3. A coluna `Survived`, tem apenas valores binários (`0` e `1`). Esta indica se a pessoa em questão sobreviveu (`1`) ou não (`0`). O objetivo da rede neuronal será prever os valores desta coluna. Esta coluna terá que ser separada das restantes, para que seja utilizada como output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-WdLyF7NHGZ"
   },
   "source": [
    "# 2. Tratamento dos dados\n",
    "Tal como referido anteriormente existem \"problemas\" nos dados, que terão de ser resolvidos antes de os aplicar numa rede neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EduZHjpTek3"
   },
   "source": [
    "## 2.1 Valores em falta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvJ6XFfRcaMI"
   },
   "source": [
    "Para saber quais as colunas que contêm valores `NaN` podemos utilizar o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7R0qSnmHcmYL",
    "outputId": "be0c6e65-c6bc-4a35-ea49-e77ebc56995f"
   },
   "outputs": [],
   "source": [
    "print(f\"Colunas com valores 'NaN': {dataset.columns[dataset.isna().any()].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9s3PhEHANHGZ"
   },
   "source": [
    "Podemos observar que existem 3 colunas com valores `NaN`: `Age`, `Cabin` e `Embarked`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qHSeQCsTo3h"
   },
   "source": [
    "### 2.1.1 Coluna 'Cabin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDUoHRhERMtw",
    "outputId": "0013e305-1c94-4979-c5dc-e2398a513e97"
   },
   "outputs": [],
   "source": [
    "missing = dataset[\"Cabin\"].isnull().sum(axis=0)\n",
    "total = dataset.shape[0]\n",
    "\n",
    "print(f\"Percentagem de valores em falta na coluna 'Cabin': {(missing*100)/total:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNsOadEPSWjc"
   },
   "source": [
    "Como 77.10 % dos dados, na coluna `Cabin`, estão em falta, iremos proceder à remoção desta coluna, pois a sua inclusão provavelmente não se iria traduzir num ganho de informação na rede neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaunDxtwNHGa"
   },
   "outputs": [],
   "source": [
    "dataset.drop(columns=['Cabin'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhcLfAWaTsnW"
   },
   "source": [
    "### 2.1.2 Coluna 'Embarked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k869BwlrNHGp",
    "outputId": "fd0b2c65-613d-4d23-ffd5-b174550bcfe0"
   },
   "outputs": [],
   "source": [
    "missing = dataset[\"Embarked\"].isnull().sum(axis=0)\n",
    "total = dataset.shape[0]\n",
    "\n",
    "print(f\"Percentagem de valores em falta na coluna 'Embarked': {(missing*100)/total:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJOXYZWgUJy5"
   },
   "source": [
    "Como nesta coluna apenas estão em falta apenas 0.22 % dos dados (o que corresponde a 2 linhas) e trata-se de uma variável categórica, iremos preencher os valores em falta com o valor mais frequente nesta coluna. Para isto iremos utilizar o módulo [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) disponibilizado pela biblioteca [scikit-learn](https://scikit-learn.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYgh4AWnU26p"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "# Criar objeto imputer com estratégia 'most_frequent'\n",
    "imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent', verbose=0)\n",
    "# Aplicar na coluna 'Embarked'. Como vamos aplicar apenas a uma coluna, e o método espera um array 2D, é necessário fazer um reshape dos dados\n",
    "dataset[\"Embarked\"] = imp_most_frequent.fit_transform(dataset[\"Embarked\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZaLvf47YVg5"
   },
   "source": [
    "### 2.1.3 Coluna 'Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MGEP4HQYkKH",
    "outputId": "9ffeb621-1e97-4a42-8e81-b1c2cd23c897"
   },
   "outputs": [],
   "source": [
    "missing = dataset[\"Age\"].isnull().sum(axis=0)\n",
    "total = dataset.shape[0]\n",
    "\n",
    "print(f\"Percentagem de valores em falta na coluna 'Age': {(missing*100)/total:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtrpVE_0YrS9"
   },
   "source": [
    "Nesta coluna estão em falta 19.87 % e trata-se de uma variável numérica, iremos preencher os valores em falta com o valor médio da coluna `Age`. Para isto iremos utilizar o mesmo módulo utilizado na coluna anterior. Existem 2 metodologias possíveis para o preenchimento de variáveis numéricas:\n",
    "1. Utilizar a média dos valores da coluna;\n",
    "2. Utilizar a mediana dos valores da coluna.\n",
    "Para decidir qual a melhor estratégia é necessário observar a curva de distribuição dos dados desta coluna (`Age`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "9y0RE2xuak9z",
    "outputId": "d6b3bd17-739e-4b17-cf8f-3b67bc47be3e"
   },
   "outputs": [],
   "source": [
    "dataset[\"Age\"].plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQv1FsJibHBc"
   },
   "source": [
    "Como é possível observar na figura acima, os dados não seguem uma distribuição normal. Assim concluímos que será mais vantajoso utilizar a mediana para o preenchimento dos valores em falta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixsI8oXNbcRc"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "# Criar objeto imputer com estratégia 'median'\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy='median', verbose=0)\n",
    "# Aplicar na coluna 'Age'. Como vamos aplicar apenas a uma coluna, e o método espera um array 2D, é necessário fazer um reshape dos dados\n",
    "dataset[\"Age\"] = imp_median.fit_transform(dataset[\"Age\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ll4HNbYkcDh9"
   },
   "source": [
    "## 2.2 Transformar valores categóricos em valores numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JsL9pdTcMwG"
   },
   "source": [
    "Para saber quais as colunas que contêm valores não numéricos podemos utilizar o seguinte código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8txtWZfic-uD",
    "outputId": "d03869cc-7f3e-4fc4-9ab0-5dd3d31f8311"
   },
   "outputs": [],
   "source": [
    "non_numeric_columns = dataset.columns.difference(dataset._get_numeric_data().columns).to_list()\n",
    "print(f\"Colunas não númericas: {non_numeric_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8Gi28npd2Nm"
   },
   "source": [
    "### 2.2.1 Coluna 'Embarked', 'Sex', 'Name', 'Ticket'\n",
    "Em primeiro lugar iremos remover as colunas `Name` e `Ticket`, pois estas são únicas dos passageiros.\n",
    "Para transformar os valores das restantes colunas (`Embarked`, `Sex`) em valores numéricos iremos utilizar o método `get_dummies()` da biblioteca `pandas`. Este método irá criar, para cada coluna, novas `n` colunas, onde `n` é o numero de diferentes classes presentes em cada coluna. Estas novas colunas irão conter valores binários que indicam se uma linha pertence ou não a essa classe. \n",
    "\n",
    "Uma outra alternativa mais simples seria transformar, para cada coluna, cada classe diferente num valor inteiro diferente. Apesar de ser mais simples, esta tranformação poderia introduzir uma falsa sensação de hierarquiedade nos dados, o que poderia levar a rede neuronal a aprender falsas suposições. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvWXyxbShK8b"
   },
   "outputs": [],
   "source": [
    "dataset.drop(columns=[\"Name\", \"Ticket\"], inplace=True)\n",
    "# Fazer OneHotEncoding para todas as colunas não numéricas, excepto a coluna 'Survived'\n",
    "dataset = pd.get_dummies(dataset, columns=[\"Embarked\", \"Sex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n96-_z-SOvI"
   },
   "source": [
    "### 2.2.2 Coluna 'Survived'\n",
    "As transformações a esta coluna são simples. Esta apenas contêm os valores `Yes` ou `No`. Apenas temos que transformar estes valores para `1` ou `0`, respetivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBaxkQ7fSx9f"
   },
   "outputs": [],
   "source": [
    "dataset[\"Survived\"] = dataset['Survived'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5vLD52VS6pS"
   },
   "source": [
    "## 2.3 Criar novos 'features' a partir de existentes\n",
    "No dataset existe a coluna `SibSp` que indica o número de irmãos/cônjuges, do passageiro, a bordo. Existe também a coluna `Parch` que indica o número de pais/filhos, do passageiro, a bordo.\n",
    "Como estas 2 colunas estão intrinsecamente relacionadas, decidimos uni-las numa só coluna, a que chamamos de `Family_Members`, que indica o número total de familiares a bordo do navio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzV6pHzVUInl"
   },
   "outputs": [],
   "source": [
    "# União das colunas\n",
    "dataset[\"Family_Members\"] = dataset[\"SibSp\"] + dataset[\"Parch\"]\n",
    "# Apagar colunas que já não são necessárias\n",
    "dataset.drop(columns=[\"SibSp\", \"Parch\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2urssVSjVwz4"
   },
   "source": [
    "## 2.4 Separação da coluna 'Survived'\n",
    "É também importante separar a coluna `Survived` do resto do dataset. Iremos criar 2 variáveis,  `X`  (irá conter os dados), e `y` (irá conter a coluna `Survived`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY6MQEkkY-6c"
   },
   "outputs": [],
   "source": [
    "X = dataset[dataset.columns.difference([\"Survived\"])]\n",
    "y = dataset[[\"Survived\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77LhuUhvUlwK"
   },
   "source": [
    "## 2.5 Normalização de dados\n",
    "Para que o modelo tenha uma maior performance, por vezes, é fundamental fazer a normalização dos dados. \n",
    "O objetivo da normalização é alterar os valores das colunas numéricas no conjunto de dados para uma escala comum, sem distorcer as diferenças no intervalos de valores.\n",
    "Neste dataset, como existe uma diferença significativa na escala dos valores, como por exemplo, na coluna `Pclass`, onde os valores variam entre 1 e 3, e as colunas `Age` e `Fare` que variam de `0` a `80` e `0` a `512`, respetivamente, é fundamental normalizar os dados pois os intervalos são muito diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMAsivYlUzXE"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Definir colunas a normalizar\n",
    "cols = [\"Pclass\", \"Age\", \"Family_Members\", \"Fare\"]\n",
    "# Normalizar os dados\n",
    "scaled_columns = StandardScaler().fit_transform(X[cols].values)\n",
    "# Juntar os dados normalizados com os outros\n",
    "X = np.concatenate([X[X.columns.difference(cols)], scaled_columns], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T3r7hkhcU-B"
   },
   "source": [
    "## 2.6 Divisão dos dados\n",
    "\n",
    "É fundamental fazer a divisão do dataset em dados de treino e dados de teste, de modo a poder avaliar a verdadeira capacidade de generalização, esta deve conseguir prever corretamente dados que nunca tenha visto.\n",
    "Uma metodologia bastante comum, é usar 80% dos dados para treinar o modelo e os restantes para avaliar a sua qualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zH3aiLe0YiLZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Dividir dados em treino e teste. \n",
    "# 20% dos dados são usados para teste\n",
    "# random_state = 42, para resultados reproduzíveis\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = global_seed, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvOs8ZyydOP1"
   },
   "source": [
    "# 3. Construir e treinar as Redes Neuronais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSw9-vFqhL4d"
   },
   "outputs": [],
   "source": [
    "# Método para apresentar diferentes métricas de avaliação dos modelos\n",
    "def print_test_results(y_test, y_predict_proba):\n",
    "    # Gerar previsões de classes\n",
    "    y_predict = (y_predict_proba > 0.5).astype(np.int32)\n",
    "    # Comparar previsões com valores reais\n",
    "    print(\"Métricas obtidas nos dados de teste:\")\n",
    "    print(f\"ROC Auc: {sk_metrics.roc_auc_score(y_test, y_predict_proba):.2f}\")\n",
    "    print(f\"Accuracy: {sk_metrics.accuracy_score(y_test, y_predict):.2f}\")\n",
    "    print(f\"F1 Score: {sk_metrics.f1_score(y_test, y_predict):.2f}\")\n",
    "    print(f\"Precision: {sk_metrics.precision_score(y_test, y_predict):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2ffRCaJYB2h"
   },
   "source": [
    "## 3.1 Construção manual de rede neuronal Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RpHBT9YTdWaq",
    "outputId": "38a8dadc-3bdc-45bc-af93-fa9a2e9f779d"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import metrics\n",
    "from sklearn import metrics as sk_metrics\n",
    "\n",
    "\n",
    "# Criar modelo sequencial\n",
    "classic_model = Sequential()\n",
    "# Criar camadas 'Dense'\n",
    "classic_model.add(Dense(8, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "classic_model.add(Dense(8, activation = 'relu'))\n",
    "# Ativação = 'sigmoid' para os valores de saída serem 0 ou 1\n",
    "classic_model.add(Dense(1, activation = 'sigmoid'))\n",
    "# Compilar modelo\n",
    "# Optimizar métrica 'AUC' para melhores resultados em classificação binária \n",
    "classic_model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "# Treinar modelo\n",
    "classic_model.fit(X_train, y_train, epochs = 120, verbose=0)\n",
    "# Fazer previsões de probabilidades\n",
    "y_predict_proba = classic_model.predict(X_test)\n",
    "# Apresentar resultados\n",
    "print_test_results(y_test, y_predict_proba)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNJBU6PfzcV5"
   },
   "source": [
    "## 3.2 Uso do `AutoKeras`\n",
    "A biblioteca `AutoKeras` é uma biblioteca de AutoML (*Automated Machine Learning*), e pretende tornar o uso de *machine learning* acessível a todos. \n",
    "\n",
    "O seu objetivo é abstrair um utilizador da escolha dos vários hyper-parâmetros de uma rede neuronal, bem como da sua arquitetura. Esta biblioteca experimenta `n` redes com configurações diferentes, e escolhe automaticamente a que melhor se adapta aos dados, ou seja a que tiver melhor capacidade de generalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJYSzrql0Ym1",
    "outputId": "ecf6009d-0077-4b07-dcf9-49a4e47e52cc"
   },
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "\n",
    "# Construir modelo\n",
    "auto_model = ak.StructuredDataClassifier(\n",
    "    overwrite=True,\n",
    "    seed=global_seed,\n",
    "    max_trials=20) # Experimenta 20 modelos diferentes\n",
    "\n",
    "# Prepapar dados\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "# Treinar modelo\n",
    "auto_model.fit(X_train, y_train, epochs=120)\n",
    "# Exportar modelo AutoKeras para modelo Keras\n",
    "auto_model = auto_model.export_model()\n",
    "\n",
    "# Prever probabilidades\n",
    "y_predict_proba = auto_model.predict(X_test)\n",
    "# Apresentar resultados\n",
    "print_test_results(y_test, y_predict_proba)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlV-lyN-XXVt"
   },
   "source": [
    "## 3.3 Comparação de resultados\n",
    "Na seguinte tabela podemos observar as diferentes métricas obtidas por ambos os modelos:\n",
    "\n",
    "| Modelo | ROC Auc | Accuracy | F1 Score | Precision\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "| Modelo `Keras` manual | 0.86 | 0.82 | **0.74** | 0.81 | \n",
    "| Modelo `AutoKeras` |  0.86 | 0.82 | 0.73 | **0.83** | \n",
    "\n",
    "Como é possível observar, ambos os modelos apresentaram resultados bastantes semelhantes, com diferenças minímas apenas no **F1 Score** e na **Precisão**.\n",
    "\n",
    "No também pode ser também interessante observar ambas as arquiteturas das duas redes criadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4tU7hrjYPTC"
   },
   "source": [
    "\n",
    "**Estrutura Primeiro modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPLbDLS_YhGP",
    "outputId": "57d70466-d03d-47ec-d2f4-60ecbedc7aaa"
   },
   "outputs": [],
   "source": [
    "# Resumir primeiro modelo\n",
    "classic_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX1hgQ6_Ymi5"
   },
   "source": [
    "**Estrutura Modelo `AutoKeras`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKrnqJELYsgw",
    "outputId": "96f9d6ac-7e74-4790-aad8-19f83359d0a3"
   },
   "outputs": [],
   "source": [
    "# Resumir modelo `AutoKeras`\n",
    "auto_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJDa0MkWoeKf"
   },
   "source": [
    "**Diferenças encontradas:**\n",
    "\n",
    "Como podemos observar a rede neuronal construída pela biblioteca `AutoKeras` apresenta um nível de complexidade muito superior à rede neuronal construída manualmente. Enquanto a primeira rede apresenta 161 parâmetros treináveis, a segunda conta com quase nove vezes mais parâmetros treináveis (1409).\n",
    "\n",
    "Portanto no presente contexto não se justificaria a escolha da segunda rede neuronal, pois em regra geral, quando 2 ou mais modelos apresentam resultados bastante idênticos, deve-se adoptar o modelo mais simples. Isto leva a que:\n",
    "* O modelo seja mais facilmente explicável;\n",
    "* Menor tempo de treino, que por sua vez leva a menores custos de computação.\n",
    "\n",
    "A adoção de uma rede neuronal construída manualmente oferece ainda a vantagem de um maior controlo sobre todo o processo. No entanto é de notar que foram experimentados apenas `20` modelos diferentes pelo `AutoKeras`. Um aumento no número de experiências realizadas poderia traduzir-se na construção de um melhor modelo. O uso do `AutoKeras` é também uma ferramenta aconselhável a iniciantes de *machine learning*, pois abstrai o utilizador dos complicados conceitos matemáticos intrínsecos deste tópico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
